# Final Project: GLUE Benchmark

## By: Tristin Johnson, Robert Hilly, Divya Parmar

For our final project, we decided to work on the General Language Understanding Evaluation (GLUE) benchmark, a collection of resources for training, evaluating, and analyzing natural language understanding systems. The GLUE benchmark includes 11 different datasets, consisting of sentence or sentence-pair language understanding tasks.

These tasks are built on established, existing datasets that cover a wide range of typical natural language processing tasks, such as sentiment analysis, textual similarity, question pairs, and textual entailment. 

We decided to work on the GLUE benchmark as the goal of this benchmark is consistent with the learning objectives of natural language processing: “The ultimate goal of GLUE is to drive research in the development of general and robust natural language understanding systems.”
	

## About the Repository



