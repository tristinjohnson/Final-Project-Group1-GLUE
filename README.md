# Final Project: GLUE Benchmark

## By: Tristin Johnson, Robert Hilly, Divya Parmar

For our final project, we decided to work on the General Language Understanding Evaluation (GLUE) benchmark, a collection of resources for training, evaluating, and analyzing natural language understanding systems. The GLUE benchmark includes 11 different datasets, consisting of sentence or sentence-pair language understanding tasks.

These tasks are built on established, existing datasets that cover a wide range of typical natural language processing tasks, such as sentiment analysis, textual similarity, question pairs, and textual entailment. 

We decided to work on the GLUE benchmark as the goal of this benchmark is consistent with the learning objectives of natural language processing: “The ultimate goal of GLUE is to drive research in the development of general and robust natural language understanding systems.”
	

## About the Repository

### 1. Code

In this directory, you will find a directory for all 11 of the GLUE Benchmark tasks. Feel free to look around and explore our models in acheiveing the scores we received. 

### 2. Group Proposal

Here, you will find our group's final project proposal, which includes all of the information regarding what we expected to get out of this project. 

### 3. Final Group Project Report

Here, you will find our group project report. Included in this report is a detailed explanation of every process we took in order to accomplish this project, such as the data preprocessing, all the information about our models, our results, and more. 

### 4. Final Group Presentation

Here is the presentation that we made, summarizing all of the work we did. It goes over each GLUE task and a quick summary ofthe models we used along with the results we received. 

### 5. Individual Final Project

In this directory, is a folder containing our names, and inside each folder is the code each one of us worked on, along with an individual report that goes into detail about the work we did individually. 



