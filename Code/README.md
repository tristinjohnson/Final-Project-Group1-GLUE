# All Code for the GLUE Benchmark

This directory is filled with all of our work in the completion of the GLUE Benchmark datasets. Each directory represents one of the 11 GLUE Datasets with our training and testing scripts. 

1. CoLA - The Corpus of Linguistic Acceptability
2. SST = The Stanford Sentiment Treebank
3. MRPC - Microsoft Research Paraphrase Corpus
4. STSB - Semantic Textual Similarity Benchmark
5. QQP - Quora Question Pairs
6. MNLI-M - MultiNLI Matched
7. MNLI-MisM - MultiNLI Mismatched
8. QNLI - Question NLI
9. RTE - Recognizing Textual Entailment
10. WNLI - Winograd NLI
11. DM - Diagnostics Main

Link to all tasks: [GLUE Benchmark Tasks](https://gluebenchmark.com/tasks)
